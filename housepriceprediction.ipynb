# housepriceprediction.ipynb

# 1. Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# Set display options for pandas
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

# 2. Load Dataset
# You can download the dataset from the link provided in README.md
# For this example, let's assume 'USA_Housing.csv' is in the same directory.
try:
    df = pd.read_csv('USA_Housing.csv')
    print("Dataset loaded successfully.")
except FileNotFoundError:
    print("Error: USA_Housing.csv not found. Please ensure the dataset is in the same directory.")
    print("Download it from: https://www.kaggle.com/datasets/vedavyasv/usa-housing")
    exit() # Exit if dataset is not found

# 3. Exploratory Data Analysis (EDA)
print("\n--- EDA ---")
print("First 5 rows of the dataset:")
print(df.head())

print("\nDataset Info:")
df.info()

print("\nMissing values:")
print(df.isnull().sum()) # Check for missing values

print("\nDescriptive statistics:")
print(df.describe())

# Drop the 'Address' column as it's likely not useful for prediction
if 'Address' in df.columns:
    df = df.drop('Address', axis=1)
    print("\n'Address' column dropped.")

# Visualize distributions (example for numerical columns)
plt.figure(figsize=(15, 10))
for i, column in enumerate(df.select_dtypes(include=np.number).columns):
    plt.subplot(3, 3, i + 1)
    sns.histplot(df[column], kde=True)
    plt.title(f'Distribution of {column}')
plt.tight_layout()
plt.show()

# Visualize correlations
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# 4. Data Preprocessing
print("\n--- Data Preprocessing ---")

# Define features (X) and target (y)
X = df.drop('Price', axis=1)
y = df['Price']

# Identify numerical and categorical features (all are numerical in this dataset)
numerical_features = X.select_dtypes(include=np.number).columns.tolist()
categorical_features = X.select_dtypes(include='object').columns.tolist() # Should be empty for USA_Housing

print(f"Numerical features: {numerical_features}")
print(f"Categorical features: {categorical_features}")

# Create preprocessing pipelines for numerical and categorical features
numerical_transformer = StandardScaler() # Standardize numerical features

# For USA_Housing, there are no categorical features, but including this for generalizability
categorical_transformer = OneHotEncoder(handle_unknown='ignore') # One-hot encode categorical features

# Create a preprocessor using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep other columns not specified
)

# 5. Split Data into Training and Testing Sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"\nTraining set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")

# 6. Model Training
print("\n--- Model Training ---")

# Define models
models = {
    'Linear Regression': LinearRegression(),
    'Random Forest Regressor': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),
    'Gradient Boosting Regressor': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

# Train and evaluate each model
trained_models = {}
for name, model in models.items():
    print(f"\nTraining {name}...")
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('regressor', model)])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"{name} Performance:")
    print(f"  MAE: {mae:.2f}")
    print(f"  MSE: {mse:.2f}")
    print(f"  RMSE: {rmse:.2f}")
    print(f"  R-squared: {r2:.2f}")

    trained_models[name] = pipeline

# Save the best performing model (e.g., Random Forest)
best_model_name = 'Random Forest Regressor' # Or choose based on R2 score
if best_model_name in trained_models:
    joblib.dump(trained_models[best_model_name], 'house_price_prediction_model.pkl')
    print(f"\n{best_model_name} model saved as house_price_prediction_model.pkl")
else:
    print(f"\nWarning: '{best_model_name}' not found in trained models. Model not saved.")


# 7. Make Predictions (Example with a new data point)
print("\n--- Making Predictions ---")

# Create a sample new data point (ensure it has the same columns as X)
# Example values, adjust as needed
sample_data = pd.DataFrame([[80000, 7, 6, 250000, 4.5]], # Avg. Area Income, Avg. Area House Age, Avg. Area Number of Rooms, Avg. Area Number of Bedrooms, Area Population
                           columns=['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',
                                    'Avg. Area Number of Bedrooms', 'Area Population'])

print("\nSample data for prediction:")
print(sample_data)

# Load the saved model
try:
    loaded_model = joblib.load('house_price_prediction_model.pkl')
    predicted_price = loaded_model.predict(sample_data)
    print(f"\nPredicted price for the sample data: ${predicted_price[0]:,.2f}")
except FileNotFoundError:
    print("\nError: Model file 'house_price_prediction_model.pkl' not found. Please train and save the model first.")
except Exception as e:
    print(f"\nAn error occurred during prediction: {e}")

# Optional: Visualize Residuals for the best model
if best_model_name in trained_models:
    best_model_pipeline = trained_models[best_model_name]
    y_pred_best = best_model_pipeline.predict(X_test)
    residuals = y_test - y_pred_best

    plt.figure(figsize=(10, 6))
    sns.histplot(residuals, kde=True)
    plt.title(f'Residuals Distribution for {best_model_name}')
    plt.xlabel('Residuals')
    plt.ylabel('Frequency')
    plt.show()

    plt.figure(figsize=(10, 6))
    plt.scatter(y_pred_best, residuals, alpha=0.5)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.title(f'Residuals vs. Predicted Values for {best_model_name}')
    plt.xlabel('Predicted Price')
    plt.ylabel('Residuals')
    plt.show()

print("\nHouse Price Prediction Notebook execution complete.")
